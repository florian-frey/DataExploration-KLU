{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "# import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Training Data\n",
    "Data source: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "- target:   The polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "- ids:      The id of the tweet\n",
    "\n",
    "- date:     The date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "- flag:     The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "- user:     The user that tweeted\n",
    "\n",
    "- text:     The text of the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = zipfile.ZipFile(\"training_data.zip\", \"r\")\n",
    "data = pd.read_csv(archive.open(\"training.1600000.processed.noemoticon.csv\"), header=None, encoding_errors=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no neutral labelled tweets.\n",
    "So we change the labels to binary form.\n",
    "(0 = negative\n",
    "1 = positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data[0]==4, 0] = 1\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For model training we only need the label and text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop([1,2,3,4], axis=1)\n",
    "data.columns = [\"label\",\"raw_text\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning everything into lowercase characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = [entry.lower() for entry in data[\"raw_text\"]]\n",
    "# data.head()\n",
    "# data[\"text\"] = data[\"raw_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing Stopwords and Links and User-Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "# we think 'no' and 'not' might be important words for the sentiment and don't want them to be removed\n",
    "stop.remove(\"no\")\n",
    "stop.remove(\"not\")\n",
    "\n",
    "# data['text'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"http[s]?://\\S+\", \"\", x))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"@\\S+\", \"\", x))\n",
    "data[\"text\"] = data[\"text\"].apply(lambda x: re.sub(\"-|\\.|,|'|\\?|\\!\", \"\", x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# sstemmer = SnowballStemmer(\"english\")\n",
    "# pstemmer = PorterStemmer()\n",
    "\n",
    "# data['text'] = data['text'].apply(lambda x: ' '.join([sstemmer.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process takes a few minutes. So you only need to do it once, save it to a csv-file and read that file\n",
    "### comment these lines after running once:\n",
    "# data[\"text\"] = [word_tokenize(entry) for entry in data[\"text\"]]\n",
    "# data.to_csv('tokenized_data.csv', index=False)\n",
    "###\n",
    "\n",
    "data = pd.read_csv('tokenized_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our finished data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>['awww', 'thats', 'a', 'bummer', 'you', 'shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>['is', 'upset', 'that', 'he', 'cant', 'update'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>['i', 'dived', 'many', 'times', 'for', 'the', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>['my', 'whole', 'body', 'feels', 'itchy', 'and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>['no', 'its', 'not', 'behaving', 'at', 'all', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                           raw_text  \\\n",
       "0      0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1      0  is upset that he can't update his Facebook by ...   \n",
       "2      0  @Kenichan I dived many times for the ball. Man...   \n",
       "3      0    my whole body feels itchy and like its on fire    \n",
       "4      0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                                text  \n",
       "0  ['awww', 'thats', 'a', 'bummer', 'you', 'shoul...  \n",
       "1  ['is', 'upset', 'that', 'he', 'cant', 'update'...  \n",
       "2  ['i', 'dived', 'many', 'times', 'for', 'the', ...  \n",
       "3  ['my', 'whole', 'body', 'feels', 'itchy', 'and...  \n",
       "4  ['no', 'its', 'not', 'behaving', 'at', 'all', ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(data[\"text\"], data[\"label\"],test_size=0.20, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "# we think 'no' and 'not' might be important words for the sentiment and don't want them to be removed\n",
    "stop.remove(\"no\")\n",
    "stop.remove(\"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(analyzer=\"word\", strip_accents=\"unicode\", stop_words=stop, min_df=10)\n",
    "Data_Tfidf = Tfidf_vect.fit_transform(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_Tfidf = Tfidf_vect.transform(data[\"text\"])\n",
    "# Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "# Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Features: 35016\n"
     ]
    }
   ],
   "source": [
    "print(\"# of Features:\", len(Tfidf_vect.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes Classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "# Naive.fit(Train_X_Tfidf, Train_Y)\n",
    "\n",
    "# predictions_NB = Naive.predict(Test_X_Tfidf)\n",
    "# print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.44991994, 0.4312098 , 0.46318412, 0.40697336, 0.41754484,\n",
       "        0.41999984, 0.43296814, 0.4352262 , 0.44605184, 0.41123199]),\n",
       " 'score_time': array([0.02795792, 0.03200078, 0.02700353, 0.03006029, 0.02797222,\n",
       "        0.03000474, 0.02901101, 0.03000593, 0.02795315, 0.02800059]),\n",
       " 'test_score': array([0.76240625, 0.7641125 , 0.7528875 , 0.76394375, 0.76575625,\n",
       "        0.756375  , 0.76336875, 0.774675  , 0.7667    , 0.75955625])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selection.cross_validate(naive_bayes.MultinomialNB(), Data_Tfidf, data[\"label\"], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Machine Classifier\n",
    "# SVM = svm.SVC(C=0.9, kernel='rbf', degree=3, gamma='auto', cache_size= 1000, max_iter=2500, decision_function_shape=\"ovo\", random_state=10)\n",
    "# SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "\n",
    "# predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# print(\"SVM Accuracy Score -> \", accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "69a1a1d853624da88fe3a3558871328a73b555428be3313eebd574666bb37328"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
